{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e226a7",
   "metadata": {},
   "source": [
    "# DETI AI Bot (FastAPI + HTML5)\n",
    "This notebook runs a complete web application.\n",
    "1. **Crawl** `ua.pt/pt/deti`\n",
    "2. **Index** into Qdrant.\n",
    "3. **Serve** an API and a Chat UI on `http://localhost:8000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fee386d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import qdrant_client\n",
    "from llama_index.core import VectorStoreIndex, Settings, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.readers.web import BeautifulSoupWebReader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- 1. SETUP & CRAWL ---\n",
    "COLLECTION_NAME = \"UA Bot\"\n",
    "LLM_MODEL=\"gemma3:latest\"\n",
    "EMB_MODEL=\"nomic-embed-text\"\n",
    "START_URL = \"https://en.wikipedia.org/wiki/University_of_Aveiro\"\n",
    "MAX_DEPTH = 2        # Requirement 2: Defined Max Depth\n",
    "MAX_PAGES = 15       # Limit to prevent infinite crawling\n",
    "\n",
    "\n",
    "Settings.llm = Ollama(model=LLM_MODEL, request_timeout=120.0,temperature=0.1,\n",
    "system_prompt=\"You are a strict assistant. Answer ONLY based on the context provided.\")\n",
    "Settings.embed_model = OllamaEmbedding(model_name=EMB_MODEL)\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "# Clean DB\n",
    "client = qdrant_client.QdrantClient(url=\"http://localhost:6333\")\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'DETI_Student_Demo_Bot/1.0 (student_project_demo; contact: your_email@example.com)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f058f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∑Ô∏è Starting Crawl from https://en.wikipedia.org/wiki/University_of_Aveiro...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78010bf6ee134ab6bf7a66e940698a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling URLs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 pages.\n",
      "['https://en.wikipedia.org/wiki/University_of_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Aveiro,_Portugal',\n",
      " 'https://en.wikipedia.org/wiki/Regi%C3%A3o_de_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Aveiro_District',\n",
      " 'https://en.wikipedia.org/wiki/Duchy_of_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Diocese_of_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Aveiro_(district)',\n",
      " 'https://en.wikipedia.org/wiki/Ria_de_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Cathedral_of_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Ovos_Moles_de_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Universidade_de_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Est%C3%A1dio_Municipal_de_Aveiro',\n",
      " 'https://en.wikipedia.org/wiki/Aveiro_Lagoon']\n"
     ]
    }
   ],
   "source": [
    "urls_to_index = []\n",
    "visited = set()\n",
    "seen_in_queue = {START_URL}\n",
    "\n",
    "print(f\"üï∑Ô∏è Starting Crawl from {START_URL}...\")\n",
    "\n",
    "queue = deque([(START_URL, 0)])\n",
    "\n",
    "with tqdm(total=MAX_PAGES, desc=\"Crawling URLs\") as pbar:\n",
    "    while queue:\n",
    "        if len(urls_to_index) >= MAX_PAGES:\n",
    "            break\n",
    "\n",
    "        url, current_depth = queue.popleft()\n",
    "        if current_depth > MAX_DEPTH and url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            time.sleep(0.25)\n",
    "            r = requests.get(url, headers=HEADERS, timeout=5)\n",
    "\n",
    "            # Skip non-HTML\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                continue\n",
    "\n",
    "            # Add to Index List\n",
    "            visited.add(url)\n",
    "            urls_to_index.append(url)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if current_depth >= MAX_DEPTH:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                full = urljoin(url, a['href']).split('#')[0]\n",
    "\n",
    "                if (\"en.wikipedia.org/wiki/\" in full\n",
    "                    and \"Aveiro\" in full\n",
    "                    and \":\" not in a['href']\n",
    "                    and full not in visited\n",
    "                    and full not in seen_in_queue):\n",
    "\n",
    "                    seen_in_queue.add(full)\n",
    "                    queue.append((full, current_depth + 1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "print(f\"Found {len(urls_to_index)} pages.\")\n",
    "pprint(urls_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584b0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f6549e11de416f8f3e9cc15bf8bcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing Content:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully Indexed 13 documents with rich metadata.\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "# Ensure we process exactly what the crawler found\n",
    "for url in tqdm(urls_to_index, desc=\"Indexing Content\"):\n",
    "    try:\n",
    "        # 1. Manual Fetch with Headers\n",
    "        r = requests.get(url, headers=HEADERS, timeout=5)\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        # 2. Extract Text & Metadata\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # Extract Text (Clean)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        clean_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "        if len(clean_text) < 50:\n",
    "            continue\n",
    "\n",
    "        # Extract Metadata Fields\n",
    "        title = soup.title.string if soup.title else \"No Title\"\n",
    "        h1 = soup.find('h1').get_text() if soup.find('h1') else \"No Header\"\n",
    "        lang = soup.html.get('lang', 'en')\n",
    "\n",
    "        # 3. Create Document with Rich Metadata\n",
    "        d = Document(\n",
    "            text=clean_text,\n",
    "            metadata={\n",
    "                \"url\": url,\n",
    "                \"title\": title.strip(),\n",
    "                \"header\": h1.strip(),\n",
    "                \"language\": lang,\n",
    "                \"content_length\": len(clean_text)\n",
    "            }\n",
    "        )\n",
    "        docs.append(d)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to index {url}: {e}\")\n",
    "\n",
    "# Build Index\n",
    "if docs:\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "    print(f\"‚úÖ Successfully Indexed {len(docs)} documents with rich metadata.\")\n",
    "else:\n",
    "    print(\"‚ùå No documents indexed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b9e301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: When was the university of Aveiro founded?\n",
      "ü§ñ Answer: The University of Aveiro was founded in 1973.\n",
      "\n",
      "--- WHAT THE LLM ACTUALLY SAW (Top Match) ---\n",
      "The rectory (built by Gon√ßalo Byrne) is located in a white building near the campus centre.\n",
      "\n",
      "The campus also has an entire Administration and Accounting Institute, which has its own service facilities and parking lots.\n",
      "\n",
      "The projects developed by UA are developed under 20 research centres, of many different scientific areas:\n",
      "\n",
      "As a research-led institution, during 2015, 316 research and technology transfer projects were active in UA. 80 of these projects are/were funded by International and European Programmes, of which 27 by the 7th Framework Programme, 13 by the Horizon 2020 and 17 by the ERASMUS +.\n",
      "\n",
      "40¬∞37‚Ä≤49‚Ä≥N 8¬∞39‚Ä≤27‚Ä≥WÔªø / Ôªø40.6303¬∞N 8.6575¬∞WÔªø / 40.6303; -8.6575...\n"
     ]
    }
   ],
   "source": [
    "# --- DEBUG CELL ---\n",
    "question = \"When was the university of Aveiro founded?\"\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "\n",
    "# Ask the engine\n",
    "response = query_engine.query(question)\n",
    "print(f\"ü§ñ Answer: {response}\\n\")\n",
    "\n",
    "print(\"--- WHAT THE LLM ACTUALLY SAW (Top Match) ---\")\n",
    "if response.source_nodes:\n",
    "    # Print the text of the top retrieved chunk\n",
    "    top_chunk = response.source_nodes[0].node.get_text()\n",
    "    print(top_chunk[:1000] + \"...\") # Print first 1000 chars\n",
    "else:\n",
    "    print(\"‚ùå No documents were retrieved! The database might be empty or search failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06cd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Server starting on http://localhost:8000\n",
      "Press 'Stop' in the notebook toolbar to shut it down.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8000\n",
      " * Running on http://192.168.17.88:8000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/Feb/2026 12:57:42] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Feb/2026 12:58:14] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Feb/2026 12:59:48] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Feb/2026 13:00:28] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Feb/2026 13:01:31] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Feb/2026 13:03:18] \"POST /api/chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# --- 3. SERVER LOGIC (FLASK VERSION) ---\n",
    "from flask import Flask, request, jsonify, make_response\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# Initialize Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Route for the UI\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    if not os.path.exists(\"index.html\"):\n",
    "        return \"<h1>Error: index.html not found. Please create it next to the notebook.</h1>\"\n",
    "\n",
    "    # Read and serve the HTML file\n",
    "    with open(\"index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "# Route for the Chat API\n",
    "@app.route(\"/api/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    try:\n",
    "        data = request.json\n",
    "        question = data.get(\"question\", \"\")\n",
    "\n",
    "        if not question:\n",
    "            return jsonify({\"answer\": \"Please ask a question.\", \"source\": None})\n",
    "\n",
    "        # Query the LlamaIndex Engine\n",
    "        # Note: This runs synchronously, blocking only this request\n",
    "        response = query_engine.query(question)\n",
    "\n",
    "        # Extract source URL if available\n",
    "        source = \"Unknown\"\n",
    "        if response.source_nodes:\n",
    "            source = response.source_nodes[0].node.metadata.get('url', 'Unknown')\n",
    "\n",
    "        return jsonify({\"answer\": str(response), \"source\": source})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return jsonify({\"answer\": \"I encountered an error processing your request.\", \"source\": \"System\"}), 500\n",
    "\n",
    "# --- 4. RUN SERVER ---\n",
    "print(\"üöÄ Server starting on http://localhost:8000\")\n",
    "print(\"Press 'Stop' in the notebook toolbar to shut it down.\")\n",
    "\n",
    "# We run Flask on port 8000.\n",
    "# debug=False is important in notebooks to prevent reloading issues.\n",
    "app.run(host=\"0.0.0.0\", port=8000, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
