{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: AI Concepts Visualized\n",
    "## 3. Text Generation with Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: The Curse of Dimensionality\n",
    "This exercise demonstrates why \"distance\" becomes meaningless in high-dimensional spaces (like the embedding space of an LLM). As we add dimensions, all points become roughly equidistant, making clustering difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Text Generation with Markov Chains\n",
    "## \"Thinking\" vs. \"Predicting Next Token\"\n",
    "\n",
    "A **Markov Chain** is a \"Small Language Model\" that predicts the next word based purely on the frequency of previous word sequences.\n",
    "\n",
    "Before the era of Transformers and ChatGPT, these probabilistic models were the standard for text generation. While modern Large Language Models (LLMs) are infinitely more complex, they share a fundamental root with Markov Chains: they are **probabilistic predictors**, not \"thinking\" machines.\n",
    "\n",
    "### 1. How It Works (The Math)\n",
    "\n",
    "Fundamentally, language modeling is about calculating the probability of the next word $w_t$ given a history of previous words.\n",
    "\n",
    "#### The Chain Rule\n",
    "In a perfect world, to predict the next word, we would look at **every** word that came before it:\n",
    "$$P(w_t | w_{t-1}, w_{t-2}, \\dots, w_1)$$\n",
    "\n",
    "#### The Markov Assumption\n",
    "However, tracking the *entire* history is computationally impossible for simple models. A Markov Chain simplifies this by assuming that the next word depends **only** on the last $N$ words (the current state).\n",
    "\n",
    "Formally, an $N$-th order Markov model approximates the probability as:\n",
    "$$P(w_t | \\text{History}) \\approx P(w_t | w_{t-1}, w_{t-2}, \\dots, w_{t-n+1})$$\n",
    "\n",
    "\n",
    "\n",
    "### 2. The Process\n",
    "\n",
    "1.  **The Corpus:** We feed the model a text (e.g., *Sherlock Holmes*).\n",
    "2.  **Training (Building the Matrix):** The model counts how often word $B$ follows word $A$.\n",
    "    * *Context:* \"The quick brown...\"\n",
    "    * *Probabilities:* $P(\\text{fox} | \\text{quick brown}) = 0.9$, $P(\\text{dog} | \\text{quick brown}) = 0.1$.\n",
    "3.  **Generation:** When asked to write, the model essentially rolls a weighted die based on these probabilities to select the next token.\n",
    "\n",
    "### 3. The \"Memory\" Parameter ($N$-Grams)\n",
    "\n",
    "The variable $N$ defines how much context the model can \"see\" when calculating probabilities.\n",
    "\n",
    "* **$N=1$ (Unigram / 0-order):** The model has no memory. It picks words based on global frequency alone.\n",
    "    * *Result:* \"The of and to a...\" (Word salad).\n",
    "* **$N=2$ (Bigram / 1st-order):** The model looks only at the **current** word to guess the next.\n",
    "    * *Math:* $P(w_t | w_{t-1})$\n",
    "    * *Result:* \"The cat sat on the moon is blue.\" (Grammatically okay locally, but nonsensical globally).\n",
    "* **$N=3$ (Trigram / 2nd-order):** The model looks at the last **2** words.\n",
    "    * *Math:* $P(w_t | w_{t-1}, w_{t-2})$\n",
    "    * *Result:* \"The cat sat on the mat.\" (Coherent sentences appear).\n",
    "\n",
    "### 4. Why This Matters: The \"Hallucination\" Trap\n",
    "\n",
    "This simple demo exposes the root cause of AI \"Hallucinations.\"\n",
    "\n",
    "The model does not know facts. It does not know what a \"cat\" is. It simply knows that statistically, maximizing $P(\\text{sat} | \\text{cat})$ yields a high score.\n",
    "\n",
    "If the training data contains misconceptions, or if the most *statistically probable* sequence of words happens to be factually incorrect, the model will generate a lie with 100% confidence. It is optimizing for **plausibility**, not **truth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    def __init__(self, n_gram=1):\n",
    "        self.n_gram = n_gram\n",
    "        self.model = {}\n",
    "        self.words = []\n",
    "\n",
    "    def fit(self, text_data):\n",
    "        self.words = text_data.replace('\\n', ' ').split()\n",
    "        self.model = {}\n",
    "        print(f\"Fitting model with N-gram size: {self.n_gram}...\")\n",
    "        for i in tqdm(range(len(self.words) - self.n_gram), desc=\"Building Chains\"):\n",
    "            # Create the state (the \"history\" of N words)\n",
    "            state = tuple(self.words[i : i + self.n_gram])\n",
    "            # The target (the next word)\n",
    "            next_word = self.words[i + self.n_gram]\n",
    "\n",
    "            if state not in self.model:\n",
    "                self.model[state] = []\n",
    "            self.model[state].append(next_word)\n",
    "\n",
    "        print(\"Training complete.\\n\")\n",
    "\n",
    "    def predict(self, length=20, start_words=None):\n",
    "        if not self.model:\n",
    "            return \"Error: Model not fitted.\"\n",
    "\n",
    "        if start_words:\n",
    "            start_tokens = start_words.split()\n",
    "            if len(start_tokens) != self.n_gram:\n",
    "                return f\"Error: Start words must contain exactly {self.n_gram} words.\"\n",
    "            current_state = tuple(start_tokens)\n",
    "            if current_state not in self.model:\n",
    "                print(f\"Warning: '{start_words}' not found in training data. Picking random start.\")\n",
    "                current_state = random.choice(list(self.model.keys()))\n",
    "        else:\n",
    "            current_state = random.choice(list(self.model.keys()))\n",
    "        output = list(current_state)\n",
    "        for _ in range(length):\n",
    "            if current_state in self.model:\n",
    "                possible_next_words = self.model[current_state]\n",
    "                next_word = random.choice(possible_next_words)\n",
    "                output.append(next_word)\n",
    "                current_state = tuple(output[-self.n_gram:])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return \" \".join(output)\n",
    "\n",
    "    def get_next_word_probabilities(self, input_gram):\n",
    "        if not self.model:\n",
    "            return None # Model not fitted\n",
    "        tokens = input_gram.split()\n",
    "        if len(tokens) != self.n_gram:\n",
    "            return None # Wrong N-gram size\n",
    "\n",
    "        state = tuple(tokens)\n",
    "        if state not in self.model:\n",
    "            return [] # State not found (valid n-gram, but no history)\n",
    "\n",
    "        possible_next_words = self.model[state]\n",
    "        total_counts = len(possible_next_words)\n",
    "        counts = Counter(possible_next_words)\n",
    "\n",
    "        results = []\n",
    "        for word, count in counts.items():\n",
    "            prob = (count / total_counts)\n",
    "            results.append({\n",
    "                \"word\": word,\n",
    "                \"probability\": prob,\n",
    "                \"count\": count,\n",
    "                \"total\": total_counts\n",
    "            })\n",
    "\n",
    "        return sorted(results, key=lambda x: x['probability'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper to load file ---\n",
    "def load_text_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Using default text.\")\n",
    "        text = \"\"\"\n",
    "        Artificial intelligence is the intelligence of machines or software, as opposed to the intelligence of humans or animals.\n",
    "        It is also the field of study in computer science that develops and studies intelligent machines.\n",
    "        \"AI\" may also refer to the machines themselves.\n",
    "        AI technology is widely used throughout industry, government, and science.\n",
    "        Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix),\n",
    "        understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art),\n",
    "        and competing at the highest level in strategic games (such as chess and Go).\n",
    "        \"\"\"\n",
    "\n",
    "    # 1. Convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation\n",
    "    # Creates a translation table mapping every punctuation char to None\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # 3. Normalize whitespace (removes tabs, newlines, and multiple spaces)\n",
    "    # split() handles \\n, \\t, and multiple spaces automatically, join() rebuilds the string cleanly\n",
    "    clean_text = \" \".join(text.split())\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def print_probability_table(phrase, probs):\n",
    "    if probs is None:\n",
    "        print(f\"Error: Could not calculate probabilities for '{phrase}' (Wrong N-gram size or model empty).\")\n",
    "        return\n",
    "\n",
    "    if len(probs) == 0:\n",
    "        print(f\"State '{phrase}' never appeared in the training text.\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Probabilities for state: '{phrase}' ---\")\n",
    "    print(f\"{'Next Word':<15} | {'Probability':<12} | {'Count':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    for item in probs:\n",
    "        p_percent = item['probability'] * 100\n",
    "        print(f\"{item['word']:<15} | {p_percent:>6.2f}%      | {item['count']}/{item['total']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded Corpus (961 words) ---\n",
      "\n",
      "Fitting model with N-gram size: 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 960/960 [00:00<00:00, 1728867.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=1 ---\n",
      "[Length 10]: ... professores cifop atual centro para implantação do quarto reitor eleito joaquim ...\n",
      "\n",
      "[Length 50]: ... equipa coordenada pelo arquiteto nuno portas alguns dos mais prestigiados arquitetos portugueses de ensino superior de ensino e formação de professores desde então não exploradas pelas instituições de professores cifop atual centro para associação académica e de cursos técnicos superiores profissionais tesp oferecidos os materiais a escola superior de aveiro mais ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Fitting model with N-gram size: 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 959/959 [00:00<00:00, 1967875.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=2 ---\n",
      "[Length 10]: ... de especialistas nacionais e estrangeiros o horizonte de formação de professores desde ...\n",
      "\n",
      "[Length 50]: ... criado o curso de estudos do ambiente e também diversos cursos de especialização tecnológica cet agora cursos técnicos superiores profissionais tesp oferecidos os dois mandatos do sétimo reitor da ua manuel antónio assunção compreendidos entre 20102014 e 20142018 são marcados pela aposta num “campus que pensa um campus que sente” e na ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Fitting model with N-gram size: 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 957/957 [00:00<00:00, 1794344.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=4 ---\n",
      "[Length 10]: ... da educação nacional josé veiga simão o seu discurso de empossamento da primeira comissão ...\n",
      "\n",
      "[Length 50]: ... projeto cinco anos volvidos sobre a fundação da universidade de aveiro um grupo de estudantes organizouse e a 28 de junho de 1978 criou a associação de estudantes da universidade de aveiro aauav nome que ainda hoje adota a fase de consolidação da universidade decorre na década de 80 em que se definiu o ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Fitting model with N-gram size: 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 953/953 [00:00<00:00, 1668268.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=8 ---\n",
      "[Length 10]: ... também diversos cursos de formação de professores ciências da natureza matemática inglêsportuguês e francêsportuguês a população estudantil era ...\n",
      "\n",
      "[Length 50]: ... currículos e metodologias inovadoras desenvolveuse desta forma uma grande área de intervenção da universidade – a educação e formação de professores foi joão evangelista loureiro antigo vicereitor da ua o grande impulsionador deste projeto cinco anos volvidos sobre a fundação da universidade de aveiro um grupo de estudantes organizouse e a 28 de junho de 1978 criou a ...\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# 1. Load Data\n",
    "# Ensure you have a 'text.txt' file in the same folder, or it will use the default text.\n",
    "text_content = load_text_file(\"../../data/ua.txt\")\n",
    "\n",
    "# 2. Define Experiments\n",
    "n_gram_settings = [1, 2, 4, 8] # N=8 might be too large for small text (overfitting/memorization)\n",
    "lengths_to_generate = [10, 50]\n",
    "\n",
    "print(f\"--- Loaded Corpus ({len(text_content.split())} words) ---\\n\")\n",
    "\n",
    "for n in n_gram_settings:\n",
    "    # Initialize and Fit\n",
    "    mk = MarkovModel(n_gram=n)\n",
    "    mk.fit(text_content)\n",
    "\n",
    "    print(f\"--- Results for N={n} ---\")\n",
    "    for l in lengths_to_generate:\n",
    "        generated = mk.predict(length=l)\n",
    "        print(f\"[Length {l}]: ... {generated} ...\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with N-gram size: 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 958/958 [00:00<00:00, 1860251.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a população estudantil era constituída por 338 estudantes e nesse mesmo ano foram construídas as primeiras infraestruturas próprias situadas onde mais tarde seria implantado o campus universitário de santiago viria a ser um contributo de qualidade para o património arquitetónico da cidade hoje podem ser admirados na ua edifícios da autoria de alcino soutinho álvaro siza vieira pedro ramalho luís ramalho josé maria lopo prata eduardo souto moura adalberto dias rebello de andrade jorge kol de carvalho gonçalo byrne e figueiredo dias anualmente visitados por centenas de especialistas nacionais e estrangeiros o horizonte de formação inicial alargouse a áreas inovadoras como o ambiente'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk = MarkovModel(n_gram=3)\n",
    "mk.fit(text_content)\n",
    "mk.predict(length=100, start_words=\"a população estudantil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Probabilities for state: 'A população estudantil' ---\n",
      "Next Word       | Probability  | Count     \n",
      "---------------------------------------------\n",
      "era             | 100.00%      | 1/1\n",
      "\n",
      "\n",
      "--- Probabilities for state: 'desta forma uma' ---\n",
      "Next Word       | Probability  | Count     \n",
      "---------------------------------------------\n",
      "grande          | 100.00%      | 1/1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs_1 = mk.get_next_word_probabilities(\"a população estudantil\")\n",
    "probs_2 = mk.get_next_word_probabilities(\"desta forma uma\")\n",
    "\n",
    "# 2. Print Afterwards\n",
    "print_probability_table(\"A população estudantil\", probs_1)\n",
    "print_probability_table(\"desta forma uma\", probs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded Corpus (1077 words) ---\n",
      "\n",
      "Fitting model with N-gram size: 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 1076/1076 [00:00<00:00, 2409541.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=1 ---\n",
      "[Length 10]: ... our world war two for virtually 100 of contributions from nato ...\n",
      "\n",
      "[Length 50]: ... americas contribution to the level of that the trump administrations investment in the results of the trump administration to germany the us paying for more than an agreement did the biggest amount on the country according to the military equipment to heavily contested areas alongside british forces in 2024 the government ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Fitting model with N-gram size: 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 1075/1075 [00:00<00:00, 1592117.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=2 ---\n",
      "[Length 10]: ... per capita casualty rates of the military alliance they didnt pay the ...\n",
      "\n",
      "[Length 50]: ... as the deployment of us troops however the agreement did not involve a transfer of sovereignty meaning greenland never became us territory is the only member of the total spent by nato countries in 2024 china generated 997 terawatthours from wind that was more than double that of the largest wind farms ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Fitting model with N-gram size: 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 1073/1073 [00:00<00:00, 1548155.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=4 ---\n",
      "[Length 10]: ... world at gansu which can be seen from space china generates more wind energy ...\n",
      "\n",
      "[Length 50]: ... for america he said weve secured commitments for a recordbreaking 18 trillion dollars and later on repeated 18 trillion dollars is invested he has made similar claims before in october he said the us had attracted investments worth 17tn £127tn but there is no publicly available evidence to support figures this big a white ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Fitting model with N-gram size: 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 1069/1069 [00:00<00:00, 1390729.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Results for N=8 ---\n",
      "[Length 10]: ... no wind farms trump also criticised wind energy a familiar target which he said was part of a ...\n",
      "\n",
      "[Length 50]: ... 18 trillion dollars and later on repeated 18 trillion dollars is invested he has made similar claims before in october he said the us had attracted investments worth 17tn £127tn but there is no publicly available evidence to support figures this big a white house website last updated in november aims to track new investment in us manufacturing ...\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# 1. Load Data\n",
    "# Ensure you have a 'text.txt' file in the same folder, or it will use the default text.\n",
    "text_content = load_text_file(\"../../data/trump.txt\")\n",
    "\n",
    "# 2. Define Experiments\n",
    "n_gram_settings = [1, 2, 4, 8] # N=8 might be too large for small text (overfitting/memorization)\n",
    "lengths_to_generate = [10, 50]\n",
    "\n",
    "print(f\"--- Loaded Corpus ({len(text_content.split())} words) ---\\n\")\n",
    "\n",
    "for n in n_gram_settings:\n",
    "    # Initialize and Fit\n",
    "    mk = MarkovModel(n_gram=n)\n",
    "    mk.fit(text_content)\n",
    "\n",
    "    print(f\"--- Results for N={n} ---\")\n",
    "    for l in lengths_to_generate:\n",
    "        generated = mk.predict(length=l)\n",
    "        print(f\"[Length {l}]: ... {generated} ...\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with N-gram size: 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Chains: 100%|██████████| 1076/1076 [00:00<00:00, 1523142.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trump total 96tn £71tn the us national security at the website for example the us bases on his administration had attracted investments his administration to be achieved by other large companies in foreign investment in the military equipment to invoke article 5 that contributed troops and infrastructure it states that but the united arab emirates uae the revenues oil and weve secured commitments for weeks trump also singled out the nazis from nato nations contributed troops and infrastructure it was part of the windfall tax on the us allies they the united arab emirates uae the international trade committee said they'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk = MarkovModel(n_gram=1)\n",
    "mk.fit(text_content)\n",
    "mk.predict(length=100, start_words=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Probabilities for state: 'trump ' ---\n",
      "Next Word       | Probability  | Count     \n",
      "---------------------------------------------\n",
      "also            |  23.08%      | 3/13\n",
      "made            |   7.69%      | 1/13\n",
      "touched         |   7.69%      | 1/13\n",
      "has             |   7.69%      | 1/13\n",
      "is              |   7.69%      | 1/13\n",
      "claimed         |   7.69%      | 1/13\n",
      "incorrectly     |   7.69%      | 1/13\n",
      "secured         |   7.69%      | 1/13\n",
      "total           |   7.69%      | 1/13\n",
      "administration  |   7.69%      | 1/13\n",
      "administrations |   7.69%      | 1/13\n",
      "\n",
      "\n",
      "--- Probabilities for state: 'is ' ---\n",
      "Next Word       | Probability  | Count     \n",
      "---------------------------------------------\n",
      "the             |  13.33%      | 2/15\n",
      "estimated       |  13.33%      | 2/15\n",
      "a               |  13.33%      | 2/15\n",
      "critical        |   6.67%      | 1/15\n",
      "talking         |   6.67%      | 1/15\n",
      "natos           |   6.67%      | 1/15\n",
      "higher          |   6.67%      | 1/15\n",
      "paid            |   6.67%      | 1/15\n",
      "due             |   6.67%      | 1/15\n",
      "invested        |   6.67%      | 1/15\n",
      "no              |   6.67%      | 1/15\n",
      "working         |   6.67%      | 1/15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs_1 = mk.get_next_word_probabilities(\"trump \")\n",
    "probs_2 = mk.get_next_word_probabilities(\"is\")\n",
    "\n",
    "# 2. Print Afterwards\n",
    "print_probability_table(\"trump \", probs_1)\n",
    "print_probability_table(\"is \", probs_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
